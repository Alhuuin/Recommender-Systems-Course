{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Recommender Systems\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"721\" alt=\"cover-image\" src=\"https://user-images.githubusercontent.com/49638680/204351915-373011d3-75ac-4e21-a6df-99cd1c552f2c.png\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "# Bayesian Personalised Ranking\n",
    "\n",
    "The aim of this lecture is to build a recommendation engine making use of the **Bayesian Personalised Ranking** (BPR) algorithm.\n",
    "\n",
    "The main inspiration to this algorithm comes from the paper [BPR: Bayesian Personalized Ranking from Implicit Feedback](https://arxiv.org/abs/1205.2618) by Steffen Rendle and from an analogous three columns architecture largely used in computer vision, called _Siamese Network_.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The BPR algorithm is a **matrix factorisation** algorithm that is based on the idea of **ranking**. The main idea is to learn a latent representation of the users and the items, such that the ranking of the items for a given user is as accurate as possible.\n",
    "\n",
    "## Encoding and Embedding\n",
    "\n",
    "Often data are not as simple as we would like them to be. To put them in a format an algorithm can digest (that is numbers) we can make use of different tools.\n",
    "\n",
    "Both _encoding_ and _embedding_ map categorical data in numerical vectors. The difference between the two is the fact that an encoder (like one-hot encoder) is a predetermined function associating a vector to each data row, while embedding vectors are low dimensional and learned. A neural network learns how to locate objects in an embedding space, placing similar entities close to each other.\n",
    "\n",
    "To summarise, an encoding maps a categorical feature in a $m$-dimensional vector, where $m$ is the number of categories of the feature. An embedding maps categorical features in an $n$-dimensional vector, where $n$ is an hyper-parameter of the model.\n",
    "\n",
    "The reason we prefer to use embeddings here is because it does not really make sense to treat each categorical value as being completely different from one another (this is one-hot-encoding). In our working case, we aim to build an hybrid recommender system to propose movies to users. Imagine you want to take into account user age range to get rid which movies they are more likely to appreciate. Is it correct to consider “equally different” a 29-years old user from a 31-years old and from a 62-years old? Embeddings will solve this issue. Indeed, we can make use of this technique to “learn” the relationships and inner connections between each possible value and our target variable.\n",
    "\n",
    "Embeddings are based on a training of a Neural Network with the categorical data, in order to retrieve weights from the Embedding layer. This allows us to have a more significant input when compared to a single One-Hot-Encoding approach, furthermore, we introduce a metric space — that is the Embedding space — in where similar entities are close.\n",
    "\n",
    "Finally, let’s have a look at our data.\n",
    "\n",
    "## The dataset\n",
    "\n",
    "For this application we use the notorious movielens dataset. We have in mind to build an hybrid recommender system, taking advantage from both users and movies features and from user/movie ratings. These data are stored in three different dataset, `users.dat` , `movies.dat` and `ratings.dat`.\n",
    "\n",
    "The [readme](http://files.grouplens.org/datasets/movielens/ml-1m-README.txt) of the dataset contains a quite exhaustive explanation of the files. User information contains gender, occupation, age range and zip-code (note: we drop the latter as it is plenty of zip-codes referring to just one user), while movie dataset contains (further than movie id) title and genre. Ratings are what will drive the supervised training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set plot parameters\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 13)\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "plt.xkcd();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and organise the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rating = pd.read_csv(\n",
    "    \"http://files.grouplens.org/datasets/movielens/ml-100k/u1.base\",\n",
    "    sep=\"\\t\",\n",
    "    engine=\"python\",\n",
    "    header=None,\n",
    ")\n",
    "df_rating.columns = [\"UserId\", \"MovieId\", \"Rating\", \"Timestamp\"]\n",
    "df_rating_test = pd.read_csv(\n",
    "    \"http://files.grouplens.org/datasets/movielens/ml-100k/u1.test\",\n",
    "    sep=\"\\t\",\n",
    "    engine=\"python\",\n",
    "    header=None,\n",
    ")\n",
    "\n",
    "df_users = pd.read_csv(\n",
    "    \"http://files.grouplens.org/datasets/movielens/ml-100k/u.user\",\n",
    "    sep=\"|\",\n",
    "    engine=\"python\",\n",
    "    header=None,\n",
    ")\n",
    "df_users.columns = [\"UserId\", \"Age\", \"Gender\", \"Occupation\", \"ZipCode\"]\n",
    "df_users.set_index(\"UserId\", inplace=True)\n",
    "df_items = pd.read_csv(\n",
    "    \"http://files.grouplens.org/datasets/movielens/ml-100k/u.item\",\n",
    "    sep=\"|\",\n",
    "    engine=\"python\",\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    header=None,\n",
    ")\n",
    "df_items.columns = [\n",
    "    \"MovieId\",\n",
    "    \"Title\",\n",
    "    \"Date\",\n",
    "    \"VideoReleaseDate\",\n",
    "    \"Url\",\n",
    "    \"unknown\",\n",
    "    \"Action\",\n",
    "    \"Adventure\",\n",
    "    \"Animation\",\n",
    "    \"Children\",\n",
    "    \"Comedy\",\n",
    "    \"Crime\",\n",
    "    \"Documentary\",\n",
    "    \"Drama\",\n",
    "    \"Fantasy\",\n",
    "    \"Film-Noir\",\n",
    "    \"Horror\",\n",
    "    \"Musical\",\n",
    "    \"Mystery\",\n",
    "    \"Romance\",\n",
    "    \"Sci-Fi\",\n",
    "    \"Thriller\",\n",
    "    \"War\",\n",
    "    \"Western\",\n",
    "]\n",
    "df_items.set_index(\"MovieId\", inplace=True)\n",
    "\n",
    "df_matrix = df_rating.pivot(index=\"UserId\", columns=\"MovieId\", values=\"Rating\")\n",
    "\n",
    "n_users = len(df_users)\n",
    "n_items = len(df_items)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the data a bit, in particular the ratings dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>MovieId</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>1673</th>\n",
       "      <th>1674</th>\n",
       "      <th>1675</th>\n",
       "      <th>1676</th>\n",
       "      <th>1677</th>\n",
       "      <th>1678</th>\n",
       "      <th>1679</th>\n",
       "      <th>1680</th>\n",
       "      <th>1681</th>\n",
       "      <th>1682</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>943 rows × 1650 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "MovieId  1     2     3     4     5     6     7     8     9     10    ...  \\\n",
       "UserId                                                               ...   \n",
       "1         5.0   3.0   4.0   3.0   3.0   NaN   4.0   1.0   5.0   NaN  ...   \n",
       "2         4.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   2.0  ...   \n",
       "3         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "4         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "5         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "...       ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "939       NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   5.0   NaN  ...   \n",
       "940       NaN   NaN   NaN   2.0   NaN   NaN   4.0   5.0   3.0   NaN  ...   \n",
       "941       5.0   NaN   NaN   NaN   NaN   NaN   4.0   NaN   NaN   NaN  ...   \n",
       "942       NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "943       NaN   5.0   NaN   NaN   NaN   NaN   NaN   NaN   3.0   NaN  ...   \n",
       "\n",
       "MovieId  1673  1674  1675  1676  1677  1678  1679  1680  1681  1682  \n",
       "UserId                                                               \n",
       "1         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "2         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "3         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "4         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "5         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "...       ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "939       NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "940       NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "941       NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "942       NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "943       NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "\n",
       "[943 rows x 1650 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_matrix\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the matrix is sparse, as the majority of couples user-movie is not rated.\n",
    "\n",
    "## Siamese Network\n",
    "\n",
    "The Siamese Network is a neural network architecture that takes as input two different instances of the same object and outputs a similarity score. The architecture is composed of two identical sub-networks, each one taking as input one of the two instances and producing an embedding vector. The similarity score is then computed as the distance between the two embedding vectors.\n",
    "\n",
    "This takes as inspiration a neural network architecture for image recognition, called [Siamese Architecture](https://en.wikipedia.org/wiki/Siamese_neural_network). According to Wikipedia, Siamese Neural Network is defined as\n",
    "\n",
    "> Siamese neural network is an artificial neural network that use the same weights while working in tandem on two different input vectors to compute comparable output vectors. Often one of the output vectors is precomputed, thus forming a baseline against which the other output vector is compared. This is similar to comparing fingerprints or more technical as a distance function for Locality-sensitive hashing.\n",
    "\n",
    "We have seen a similar architecture in the previous lecture, where we used it to compute the compatibility between a user and a movie. In this case, we will use it to compute the \"unsimilarity\" between two movies.\n",
    "\n",
    "The idea comes from the context of computer vision, where the Siamese Network is used to compute the similarity between two images. In this case, the network is trained to output a high value if the two images are similar, and a low value otherwise. In our case, we want to do the opposite: we want to output a high value if the two movies are \"different\", and a low value otherwise.\n",
    "\n",
    "Let's see how the architecture looks like in the case of image similarity.\n",
    "\n",
    "### Siamese Network for images\n",
    "\n",
    "The main idea is to have two identical sub-networks, each one taking as input one of the two images and producing an embedding vector. The similarity score is then computed as the distance between the two embedding vectors.\n",
    "\n",
    "The image below describes how to get the embedding vector for a single image.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://raw.githubusercontent.com/oscar-defelice/DeepLearning-lectures/master/src/datasets/FaceRec/images/f_x.png\" style=\"width:380px;height:150px;\">\n",
    "</p>\n",
    "\n",
    "Siamese Network architecture for image similarity works as follows:\n",
    "\n",
    "- Input: two images $x_1$ and $x_2$.\n",
    "- Output: a similarity score $s$.\n",
    "\n",
    "The similarity score is computed as the distance between the two embedding vectors $f(x_1)$ and $f(x_2)$.\n",
    "The aim of such a network is _image recognition_, that is to say that we want to train the network to output a high value if the two images have the same content (the same person for instance), and a low value otherwise.\n",
    "\n",
    "#### The triplet loss\n",
    "\n",
    "The triplet loss is a loss function that is used to train a neural network to learn an embedding space in which similar entities are close to each other and dissimilar entities are far apart. The loss is computed on triplets of entities, where each triplet is composed of an _anchor_ entity, a _positive_ entity and a _negative_ entity.\n",
    "The anchor and the positive entity are similar, while the anchor and the negative entity are dissimilar.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"721\" alt=\"siamese-network\" src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H-gKds4bIXuGs7TvuusY5w.png\">\n",
    "</p>\n",
    "\n",
    "The idea is the following, we want to train a network to produce encodings for images. An encoding is a good one if:\n",
    "\n",
    "1. The encodings of two images of the same person are quite similar to each other.\n",
    "2. The encodings of two images of different persons are very different.\n",
    "\n",
    "The triplet loss function formalises this, and tries to \"push\" the encodings of two images of the same person (Anchor and Positive) closer together, while \"pulling\" the encodings of two images of different persons (Anchor, Negative) further apart.\n",
    "\n",
    "The mathematical formulation of the triplet loss is the following:\n",
    "\n",
    "$$ \\mathcal{L}(A, P, N) = \\max\\left(\\delta(A, P) - \\delta(A, N) + \\alpha, 0 \\right)\\, , $$\n",
    "\n",
    "where $\\delta$ is the distance function (e.g. Euclidean distance), $\\alpha$ is a margin, and $A$, $P$ and $N$ are the anchor, positive and negative images respectively.\n",
    "In training, we want to minimise the triplet loss, that is to say that we want to find the parameters of the network that minimise the triplet loss.\n",
    "\n",
    "A recommender system can be seen in the same way.\n",
    "We have an anchor user, and two recommended items, positive and negative, meaning the recommendation should associate the user to the positive item and penalise the negative one.\n",
    "A modified version of the triplet loss is a good way to train the network to produce encodings for users and items, and to learn a good metric space in which similar users and items are close to each other and dissimilar users and items are far apart.\n",
    "\n",
    "#### Training the Siamese Network\n",
    "\n",
    "As mentioned above, the triplet loss function suggests us to train the network to produce encodings for users and items by feeding it triplets of users, positive and negative items.\n",
    "\n",
    "The crucial part here is that all the three branches of the network share the _same weights_. This means, that the encodings are equivalent for all the branches, and that the network learns a good metric space in which similar users and items are close to each other and dissimilar users and items are far apart.\n",
    "\n",
    "### Siamese Network for recommendations\n",
    "\n",
    "Here how we are going to use the information stored in rating matrix. Our training set will be made of triplets [user, liked movie, not liked movie].\n",
    "\n",
    "Formally the architecture is really similar to the one we used for images, the only difference is that we have to deal with difference sources of data. We will use the embedding layer to encode users and items data.\n",
    "\n",
    "#### Bayesian Personalised Ranking\n",
    "\n",
    "We have already said that [Bayesian Personalised Ranking](https://arxiv.org/abs/1205.2618) (BPR) is a matrix factorisation algorithm to exploit latent vectors of users and items.\n",
    "Here we want to stress that one can also see BPR as a modified version of the triplet loss.\n",
    "The idea is to train a model to predict the preference of a user for an item, given the preference of the same user for another item. The model is trained by maximizing the probability of the correct item being ranked higher than the incorrect item.\n",
    "\n",
    "Mathematically, the BPR loss is defined as:\n",
    "\n",
    "$$ \\mathcal{L}(A, P, N) = 1 - \\sigma(f(A)\\cdot f(P) - f(A)\\cdot f(N))\\, , $$\n",
    "\n",
    "where $\\sigma$ is the sigmoid function, while $f$ is the function that maps users and items to a latent space, _i.e._ the embedding map.\n",
    "Recall the latent space is a vector space in which similar users and items are close to each other and dissimilar users and items are far apart.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7WT2-VtOrFXB-5u7K6KXOQ.png\" style=\"width:980px\">\n",
    "</p>\n",
    "\n",
    "#### Training the Siamese Network\n",
    "\n",
    "Formally the network is really similar to the one we used for images, the only difference is that we have to deal with difference sources of data. We will use the embedding layer to encode users and items data, after that the network layers are the same and they share the same weights.\n",
    "The loss function we want to minimise is the BPR loss we saw above.\n",
    "\n",
    "It is noteworthy that we are now able — in a quite simple way — to leverage user and item features but also to take into account previously expressed feedbacks. This is precisely an hybrid recommender system. I think this may express clearly the power of the embedding tool.\n",
    "\n",
    "### The model\n",
    "\n",
    "Let’s put all of this in a neural network model. We are going to use Keras functional API.\n",
    "\n",
    "First we need to create a network to be trained.\n",
    "Keras does not have a triplet loss function defined, so we have to define a custom one.\n",
    "There are several ways to do so:\n",
    "\n",
    "1. Create a custom loss function\n",
    "2. Create a custom layer\n",
    "\n",
    "We are going to follow the second approach, however, we will strongly encourage you to read the [official documentation](https://keras.io/api/losses/) to understand how to create a custom loss function and do it as an exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triplet loss layer\n",
    "\n",
    "\n",
    "class TripletLossLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Layer object to minimise the triplet loss.\n",
    "    Here we implement the Bayesian Personal Ranking triplet loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TripletLossLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def bpr_triplet_loss(self, inputs):\n",
    "        \"\"\"\n",
    "        Bayesian Personal Ranking triplet loss.\n",
    "        We actually use log-loss for numerical purposes.\n",
    "        \"\"\"\n",
    "        anchor, positive, negative = inputs\n",
    "        p_dist = tf.math.reduce_sum(anchor * positive, axis=-1, keepdims=True)\n",
    "        n_dist = tf.math.reduce_sum(anchor * negative, axis=-1, keepdims=True)\n",
    "        return tf.math.log(1.0 - tf.math.sigmoid(p_dist - n_dist))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        loss = self.bpr_triplet_loss(inputs)\n",
    "        self.add_loss(loss)\n",
    "        return loss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a `ScoreLayer` to compute the score of compatibility of a user and an item.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Layer object to predict positive matches.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ScoreLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def rec_similarity(self, inputs):\n",
    "        \"\"\"\n",
    "        rec_similarity function\n",
    "        \"\"\"\n",
    "        anchor, item = inputs\n",
    "        score = tf.keras.layers.Dot(axes=1)([anchor, item])\n",
    "        return score\n",
    "\n",
    "    def call(self, inputs):\n",
    "        pred = self.rec_similarity(inputs)\n",
    "        return pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing\n",
    "\n",
    "We need to preprocess the data to feed the network.\n",
    "We are going to define some functions to encode users and items data.\n",
    "\n",
    "First, we define two functions that will allow us to extract negative and positive items given a user id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get positive items of a given user\n",
    "def get_pos(df, anchor):\n",
    "    \"\"\"\n",
    "    Given a user id anchor, it gives back the list of liked item ids.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        df : Pandas DataFrame\n",
    "                Dataframe containing ratings, having user id as rows, movie id as columns\n",
    "\n",
    "        anchor : int\n",
    "                    user id to be serving as anchor.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "        pos_list : list of int\n",
    "                    list of P elements\n",
    "                    values : Positive (Movie id evaluated at least 4.0)\n",
    "    \"\"\"\n",
    "    POS_THR = 4.0\n",
    "\n",
    "    ps = df.loc[anchor] >= POS_THR\n",
    "    return ps[ps].index.values\n",
    "\n",
    "\n",
    "# Get negative items of a given user\n",
    "def get_neg(df, anchor):\n",
    "    \"\"\"\n",
    "    Given a user id anchor, it gives back the list of unliked item ids.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        df : Pandas DataFrame\n",
    "                Dataframe containing ratings, having user id as rows, movie id as columns\n",
    "\n",
    "        anchor : int\n",
    "                    user id to be serving as anchor.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "        neg_list : list of int\n",
    "                    list of N elements\n",
    "                    values : Negative (Movie id evaluates at most 2.0)\n",
    "    \"\"\"\n",
    "    NEG_THR = 3.0\n",
    "\n",
    "    ps = df.loc[anchor] <= NEG_THR\n",
    "    return ps[ps].index.values\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we need to encode categorical features. We are going to use the `OneHotEncoder` from scikit-learn to encode users and items features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_con_features = [\"Age\"]  # Continuous features\n",
    "user_cat_features = [\"Gender\", \"Occupation\"]  # Categorical features\n",
    "\n",
    "item_features = [\n",
    "    \"Action\",\n",
    "    \"Adventure\",\n",
    "    \"Animation\",\n",
    "    \"Children\",\n",
    "    \"Comedy\",\n",
    "    \"Crime\",\n",
    "    \"Documentary\",\n",
    "    \"Drama\",\n",
    "    \"Fantasy\",\n",
    "    \"Film-Noir\",\n",
    "    \"Horror\",\n",
    "    \"Musical\",\n",
    "    \"Mystery\",\n",
    "    \"Romance\",\n",
    "    \"Sci-Fi\",\n",
    "    \"Thriller\",\n",
    "    \"War\",\n",
    "    \"Western\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess arrays\n",
    "def process_features(\n",
    "    df: pd.DataFrame, con_features: List[str], cat_features: List[str]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Given a dataframe, it gives back corresponding feature vectors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        df : Pandas DataFrame\n",
    "                  Dataframe containing metadata.\n",
    "        cat_features : list of str\n",
    "                        list of categorical variables, columns of df.\n",
    "        con_features : list of str\n",
    "                        list of continuous variables, columns of df.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "        X : array of shape (len(df), n_encoded_features)\n",
    "                    X is a matrix having as rows id feature vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialise an empty lists of vectors\n",
    "    X_cat = []\n",
    "    X_con = []\n",
    "\n",
    "    # for each continuous feature, add it as it is.\n",
    "    for column in con_features:\n",
    "        X = np.array(df[column])\n",
    "        X_con.append(X)\n",
    "\n",
    "    # for each categorical feature, get the numerical encoding of the feature vector\n",
    "    for column in cat_features:\n",
    "        X = np.asarray(df[column].tolist())\n",
    "        X_line = pd.factorize(X)[0]\n",
    "        X_cat.append(np.asarray(X_line))\n",
    "\n",
    "    # transform lists in arrays\n",
    "    try:\n",
    "        X_con = np.column_stack(X_con)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        X_cat = np.column_stack(X_cat)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # concatenate arrays\n",
    "    if (len(cat_features) > 0) and (len(con_features) > 0):\n",
    "        X = np.concatenate((X_con, X_cat), axis=1)\n",
    "    elif len(cat_features) == 0:\n",
    "        X = X_con\n",
    "    elif len(con_features) == 0:\n",
    "        X = X_cat\n",
    "    # return the encoded features\n",
    "    return X\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This weird function allows us to preprocess both users and items data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_usr = process_features(df_users, user_con_features, user_cat_features)\n",
    "X_item = process_features(df_items, item_features, [])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building\n",
    "\n",
    "We are going to build a Siamese Network thanks to the layers we defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    n_users: int,\n",
    "    n_user_features: int,\n",
    "    n_items: int,\n",
    "    n_item_features: int,\n",
    "    emb_dim: int = 32,\n",
    ") -> Tuple[tf.keras.Model, tf.keras.Model]:\n",
    "    \"\"\"\n",
    "    Define the Model for training.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    n_users : int\n",
    "                number of users\n",
    "    n_items : int\n",
    "                number of items\n",
    "    emb_dim : int\n",
    "                dimension of the embedding space\n",
    "                default = 32\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    model_train : tf.keras.Model\n",
    "                Model instance to be trained\n",
    "    model_predict : tf.keras.Model\n",
    "                Model instance to be used for predictions\n",
    "    \"\"\"\n",
    "    ### Input Layers\n",
    "    user_input = tf.keras.Input((n_user_features,), name=\"user_input\")\n",
    "    positive_item_input = tf.keras.Input((n_item_features,), name=\"pos_item_input\")\n",
    "    negative_item_input = tf.keras.Input((n_item_features,), name=\"neg_item_input\")\n",
    "\n",
    "    inputs = [user_input, positive_item_input, negative_item_input]\n",
    "\n",
    "    ### Embedding Layers\n",
    "    # User embedding\n",
    "    user_embedding = tf.keras.layers.Embedding(\n",
    "        n_users, emb_dim, input_length=n_user_features, name=\"user_embedding\"\n",
    "    )\n",
    "    # Positive and negative items will share the same embedding\n",
    "    item_embedding = tf.keras.layers.Embedding(\n",
    "        n_items, emb_dim, input_length=n_item_features, name=\"item_embedding\"\n",
    "    )\n",
    "    # Layers to convert embedding vectors in the same dimensional vectors\n",
    "    vec_conv64 = tf.keras.layers.Dense(64, name=\"dense_vec64\", activation=\"relu\")\n",
    "    vec_conv32 = tf.keras.layers.Dense(32, name=\"dense_vec32\", activation=\"relu\")\n",
    "    vec_conv = tf.keras.layers.Dense(emb_dim, name=\"dense_vec\", activation=\"softmax\")\n",
    "\n",
    "    # Anchor (the user) embedding\n",
    "    user_vec = tf.keras.layers.Flatten()(user_embedding(user_input))\n",
    "    user_vec = tf.keras.layers.Dense(emb_dim, name=\"dense_user\", activation=\"softmax\")(\n",
    "        user_vec\n",
    "    )\n",
    "\n",
    "    # Positive item embedding\n",
    "    pos_item_vec = tf.keras.layers.Flatten()(item_embedding(positive_item_input))\n",
    "    pos_item_vec = vec_conv64(pos_item_vec)\n",
    "    pos_item_vec = vec_conv32(pos_item_vec)\n",
    "    pos_item_vec = vec_conv(pos_item_vec)\n",
    "\n",
    "    # Negative item embedding\n",
    "    neg_item_vec = tf.keras.layers.Flatten()(item_embedding(negative_item_input))\n",
    "    neg_item_vec = vec_conv64(neg_item_vec)\n",
    "    neg_item_vec = vec_conv32(neg_item_vec)\n",
    "    neg_item_vec = vec_conv(neg_item_vec)\n",
    "\n",
    "    ### Score Layers\n",
    "    p_rec_score = ScoreLayer(name=\"pos_recommendation_score\")([user_vec, pos_item_vec])\n",
    "    n_rec_score = ScoreLayer(name=\"neg_recommendation_score\")([user_vec, neg_item_vec])\n",
    "\n",
    "    ### Triplet Loss Layer\n",
    "    loss = TripletLossLayer(name=\"triple_loss\")([user_vec, pos_item_vec, neg_item_vec])\n",
    "\n",
    "    # Connect the inputs with the outputs\n",
    "    network_train = tf.keras.Model(inputs=inputs, outputs=loss, name=\"training_model\")\n",
    "\n",
    "    network_predict = tf.keras.Model(\n",
    "        inputs=inputs[:-1], outputs=p_rec_score, name=\"inference_model\"\n",
    "    )\n",
    "\n",
    "    # return the model\n",
    "    return network_train, network_predict\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "We are now ready to train the model. We are going to use the `fit` method of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"training_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " pos_item_input (InputLayer)    [(None, 18)]         0           []                               \n",
      "                                                                                                  \n",
      " neg_item_input (InputLayer)    [(None, 18)]         0           []                               \n",
      "                                                                                                  \n",
      " item_embedding (Embedding)     (None, 18, 32)       53824       ['pos_item_input[0][0]',         \n",
      "                                                                  'neg_item_input[0][0]']         \n",
      "                                                                                                  \n",
      " user_input (InputLayer)        [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 576)          0           ['item_embedding[0][0]']         \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 576)          0           ['item_embedding[1][0]']         \n",
      "                                                                                                  \n",
      " user_embedding (Embedding)     (None, 3, 32)        30176       ['user_input[0][0]']             \n",
      "                                                                                                  \n",
      " dense_vec64 (Dense)            (None, 64)           36928       ['flatten_4[0][0]',              \n",
      "                                                                  'flatten_5[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 96)           0           ['user_embedding[0][0]']         \n",
      "                                                                                                  \n",
      " dense_vec32 (Dense)            (None, 32)           2080        ['dense_vec64[0][0]',            \n",
      "                                                                  'dense_vec64[1][0]']            \n",
      "                                                                                                  \n",
      " dense_user (Dense)             (None, 32)           3104        ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_vec (Dense)              (None, 32)           1056        ['dense_vec32[0][0]',            \n",
      "                                                                  'dense_vec32[1][0]']            \n",
      "                                                                                                  \n",
      " triple_loss (TripletLossLayer)  (None, 1)           0           ['dense_user[0][0]',             \n",
      "                                                                  'dense_vec[0][0]',              \n",
      "                                                                  'dense_vec[1][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 127,168\n",
      "Trainable params: 127,168\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network_train, network_predict = build_model(\n",
    "    n_users=n_users,\n",
    "    n_user_features=X_usr.shape[1],\n",
    "    n_items=n_items,\n",
    "    n_item_features=X_item.shape[1],\n",
    "    emb_dim=32,\n",
    ")\n",
    "optimiser = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "network_train.compile(loss=None, optimizer=optimiser)\n",
    "network_train.summary()\n",
    "tf.keras.utils.plot_model(\n",
    "    network_train,\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    to_file=\"../images/bpr_model.png\",\n",
    ")\n",
    "n_iteration = 0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some comments\n",
    "\n",
    "At this stage, It is good to take a breath and have a look at the code defining the model above.\n",
    "\n",
    "We made use of Keras functional API to build the network.\n",
    "Note how we defined layers before applying to other layer objects.\n",
    "This allows us to have weights shared by positive and negative items.\n",
    "\n",
    "A further noteworthy aspect of the function above is that it returns actually two models: one to train, the other one to predict.\n",
    "This because, as mentioned above, BPR Triplet Loss is not something already implemented in Keras.\n",
    "To be sure the two returned models are actually the same except for the last layer.\n",
    "\n",
    "Indeed, to verify that the two networks are actually sharing the same weights we can call the `layers` attribute of the two models and seeing they are actually pointing at the same addresses in memory.\n",
    "\n",
    "This is important because we need to train our network for one-shot learning meaning, it has to correctly predict preferences even for a never-seen user or an unknown item.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x2c5f01570>,\n",
       " <keras.engine.input_layer.InputLayer at 0x2c5f004f0>,\n",
       " <keras.layers.core.embedding.Embedding at 0x2c5f00850>,\n",
       " <keras.engine.input_layer.InputLayer at 0x2c5f019f0>,\n",
       " <keras.layers.reshaping.flatten.Flatten at 0x29882cfd0>,\n",
       " <keras.layers.reshaping.flatten.Flatten at 0x2c4a5cf70>,\n",
       " <keras.layers.core.embedding.Embedding at 0x2c5f00910>,\n",
       " <keras.layers.core.dense.Dense at 0x2c5f00820>,\n",
       " <keras.layers.reshaping.flatten.Flatten at 0x2c5f02470>,\n",
       " <keras.layers.core.dense.Dense at 0x2c5f003d0>,\n",
       " <keras.layers.core.dense.Dense at 0x2c4a5de70>,\n",
       " <keras.layers.core.dense.Dense at 0x2c5f00ca0>,\n",
       " <__main__.TripletLossLayer at 0x2c4a5ebf0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_train.layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x2c5f01570>,\n",
       " <keras.layers.core.embedding.Embedding at 0x2c5f00850>,\n",
       " <keras.engine.input_layer.InputLayer at 0x2c5f019f0>,\n",
       " <keras.layers.reshaping.flatten.Flatten at 0x29882cfd0>,\n",
       " <keras.layers.core.embedding.Embedding at 0x2c5f00910>,\n",
       " <keras.layers.core.dense.Dense at 0x2c5f00820>,\n",
       " <keras.layers.reshaping.flatten.Flatten at 0x2c5f02470>,\n",
       " <keras.layers.core.dense.Dense at 0x2c5f003d0>,\n",
       " <keras.layers.core.dense.Dense at 0x2c4a5de70>,\n",
       " <keras.layers.core.dense.Dense at 0x2c5f00ca0>,\n",
       " <__main__.ScoreLayer at 0x2c50684c0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_predict.layers\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "We are now ready to pass to the training part of our recommender system.\n",
    "\n",
    "#### Construction of training triplets\n",
    "\n",
    "Before going and train this model we need to build the batches. Recall our training set is made by triplets (_user_, _positive movie_, _negative movie_).\n",
    "\n",
    "As one can see from the code above, the model expects a collection of three arrays as input. Corresponding to the feature vectors of users and items.\n",
    "\n",
    "Hence, the batch composer function will take information from the users dataset, from movies dataset and from the ratings matrix to compose triplets.\n",
    "\n",
    "Thus, the function will return a list containing 3 batch_size-long arrays, representing the triplets. In other words we have a list of objects\n",
    "\n",
    "$$ [a, p, n] $$\n",
    "\n",
    "where $a$ is the anchor user, $p$ is the positive item and $n$ is the negative item.\n",
    "\n",
    "where $a$ is an array of shape (`batch_size`, `n_user_features`), $p$ and $n$ are both arrays of shape (`batch_size`, `n_item_features`).\n",
    "\n",
    "Each row of the array corresponds to a feature vector.\n",
    "Indeed the triplet referring to the first user (coupled to a movie evaluated positively and to a one negatively ranked) is for example,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([24,  0,  0]),\n",
       " array([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_usr[0], X_item[0], X_item[8]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get to these results, we need to define some functions.\n",
    "\n",
    "#### Cosine distance\n",
    "\n",
    "First, we need an auxiliary function to compute the cosine distance between two vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_dist(x, y, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Cosine distance between vectors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy array\n",
    "    y : numpy array\n",
    "    eps : float\n",
    "            little number to avoid zero division error.\n",
    "            default : 1e-6\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    dist = 1 - (x.y)/(||x||||y||)\n",
    "    \"\"\"\n",
    "    return 1 - np.dot(x, y.T) / (np.linalg.norm(x) * np.linalg.norm(y) + eps)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translating ratings matrix into triplets\n",
    "\n",
    "The function randomly select a user and among its positive/negative ratings randomly picks two to compose the triplet.\n",
    "\n",
    "This is the function `build_batch`.\n",
    "Note how triplets are chosen randomly, we will see how this is a really inefficient way to train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch(batch_size, X_usr, X_item, df, return_cache=False):\n",
    "    \"\"\"\n",
    "    Returns the list of three arrays to feed the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size : int\n",
    "                  size of the batch.\n",
    "\n",
    "    X_usr : numpy array of shape (n_users, n_user_features)\n",
    "                array of user metadata.\n",
    "\n",
    "    X_item : numpy array of shape (n_items, n_item_features)\n",
    "                array of item metadata.\n",
    "\n",
    "    df : Pandas DataFrame\n",
    "            dataframe containing user-item ratings.\n",
    "\n",
    "    return_cache : bool\n",
    "                    parameter to trigger whether we want the list of ids corresponding to triplets.\n",
    "                    default: False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    triplets : list of numpy arrays\n",
    "                list containing 3 tensors A,P,N corresponding to:\n",
    "                    - Anchor A : (batch_size, n_user_features)\n",
    "                    - Positive P : (batch_size, n_item_features)\n",
    "                    - Negative N : (batch_size, n_item_features)\n",
    "    \"\"\"\n",
    "    # constant values\n",
    "    n_user_features = X_usr.shape[1]\n",
    "    n_item_features = X_item.shape[1]\n",
    "\n",
    "    # define user_list\n",
    "    user_list = list(df.index.values)\n",
    "\n",
    "    # initialise result\n",
    "    triplets = [\n",
    "        np.zeros((batch_size, n_user_features)),  # anchor\n",
    "        np.zeros((batch_size, n_item_features)),  # pos\n",
    "        np.zeros((batch_size, n_item_features)),  # neg\n",
    "    ]\n",
    "    user_ids = []\n",
    "    p_ids = []\n",
    "    n_ids = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # pick one random user for anchor\n",
    "        anchor_id = random.choice(user_list)\n",
    "        user_ids.append(anchor_id)\n",
    "\n",
    "        # all possible positive/negative samples for selected anchor\n",
    "        p_item_ids = get_pos(df, anchor_id)\n",
    "        n_item_ids = get_neg(df, anchor_id)\n",
    "\n",
    "        # pick one of the positve ids\n",
    "        try:\n",
    "            positive_id = random.choice(p_item_ids)\n",
    "        except IndexError:\n",
    "            positive_id = 0\n",
    "\n",
    "        p_ids.append(positive_id)\n",
    "\n",
    "        # pick one of the negative ids\n",
    "        try:\n",
    "            negative_id = random.choice(n_item_ids)\n",
    "        except IndexError:\n",
    "            negative_id = 0\n",
    "\n",
    "        n_ids.append(negative_id)\n",
    "\n",
    "        # define triplet\n",
    "        triplets[0][i, :] = X_usr[anchor_id - 1][:]\n",
    "\n",
    "        if positive_id == 0:\n",
    "            triplets[1][i, :] = np.zeros((n_item_features,))\n",
    "        else:\n",
    "            triplets[1][i, :] = X_item[positive_id - 1][:]\n",
    "\n",
    "        if negative_id == 0:\n",
    "            triplets[2][i, :] = np.zeros((n_item_features,))\n",
    "        else:\n",
    "            triplets[2][i, :] = X_item[negative_id - 1][:]\n",
    "\n",
    "    if return_cache:\n",
    "        cache = {\"users\": user_ids, \"positive\": p_ids, \"negative\": n_ids}\n",
    "        return triplets, cache\n",
    "\n",
    "    return triplets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the triplets to be _hard_, meaning that we want the model to be able to discriminate between items that might seem similar.\n",
    "This because we do not want to fall into the margin condition of the BPR loss function, where the model is not able to learn anything from a \"too easy\" triplet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triplets_hard(batch_size, X_usr, X_item, df, return_cache=False):\n",
    "    \"\"\"\n",
    "    Returns the list of three arrays to feed the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        batch_size : int\n",
    "                        size of the batch.\n",
    "        X_usr : numpy array of shape (n_users, n_user_features)\n",
    "                        array of user metadata.\n",
    "        X_item : numpy array of shape (n_items, n_item_features)\n",
    "                        array of item metadata.\n",
    "        df : Pandas DataFrame\n",
    "                dataframe containing user-item ratings.\n",
    "        return_cache : bool\n",
    "                        parameter to trigger whether we want the list of ids corresponding to\n",
    "                        triplets.\n",
    "                        default: False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        triplets : list of numpy arrays\n",
    "                    list containing 3 tensors A,P,N corresponding to:\n",
    "                        - Anchor A : (batch_size, n_user_features)\n",
    "                        - Positive P : (batch_size, n_item_features)\n",
    "                        - Negative N : (batch_size, n_item_features)\n",
    "    \"\"\"\n",
    "    # constant values\n",
    "    n_user_features = X_usr.shape[1]\n",
    "    n_item_features = X_item.shape[1]\n",
    "\n",
    "    # define user_list\n",
    "    user_list = list(df.index.values)\n",
    "\n",
    "    # initialise result\n",
    "    triplets = [\n",
    "        np.zeros((batch_size, n_user_features)),  # anchor\n",
    "        np.zeros((batch_size, n_item_features)),  # positive\n",
    "        np.zeros((batch_size, n_item_features)),  # negative\n",
    "    ]\n",
    "    user_ids = []\n",
    "    p_ids = []\n",
    "    n_ids = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # pick one random user for anchor\n",
    "        anchor_id = random.choice(user_list)\n",
    "        user_ids.append(anchor_id)\n",
    "\n",
    "        # all possible positive/negative samples for selected anchor\n",
    "        p_item_ids = get_pos(df, anchor_id)\n",
    "        n_item_ids = get_neg(df, anchor_id)\n",
    "\n",
    "        # pick one of the positve ids\n",
    "        try:\n",
    "            positive_id = random.choice(p_item_ids)\n",
    "        except IndexError:\n",
    "            positive_id = 0\n",
    "\n",
    "        p_ids.append(positive_id)\n",
    "\n",
    "        # pick the most similar negative id\n",
    "        try:\n",
    "            n_min = np.argmin(\n",
    "                [\n",
    "                    (cosine_dist(X_item[positive_id - 1], X_item[k - 1]))\n",
    "                    for k in n_item_ids\n",
    "                ]\n",
    "            )\n",
    "            negative_id = n_item_ids[n_min]\n",
    "        except:\n",
    "            try:\n",
    "                negative_id = random.choice(n_item_ids)\n",
    "            except IndexError:\n",
    "                negative_id = 0\n",
    "\n",
    "        n_ids.append(negative_id)\n",
    "\n",
    "        # define triplet\n",
    "        triplets[0][i, :] = X_usr[anchor_id - 1][:]\n",
    "\n",
    "        if positive_id == 0:\n",
    "            triplets[1][i, :] = np.zeros((n_item_features,))\n",
    "        else:\n",
    "            triplets[1][i, :] = X_item[positive_id - 1][:]\n",
    "\n",
    "        if negative_id == 0:\n",
    "            triplets[2][i, :] = np.zeros((n_item_features,))\n",
    "        else:\n",
    "            triplets[2][i, :] = X_item[negative_id - 1][:]\n",
    "\n",
    "    if return_cache:\n",
    "        cache = {\"users\": user_ids, \"positive\": p_ids, \"negative\": n_ids}\n",
    "        return triplets, cache\n",
    "\n",
    "    return triplets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[51.,  0.,  7.]]),\n",
       "  array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "          0., 0.]]),\n",
       "  array([[0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0.]])],\n",
       " {'users': [572], 'positive': [319], 'negative': [924]})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_batch(batch_size=1, X_usr=X_usr, X_item=X_item, df=df_matrix, return_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[50.,  1.,  2.]]),\n",
       "  array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0.]]),\n",
       "  array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "          0., 0.]])],\n",
       " {'users': [503], 'positive': [484], 'negative': [603]})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_triplets_hard(\n",
    "    batch_size=1, X_usr=X_usr, X_item=X_item, df=df_matrix, return_cache=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model over batches\n",
    "\n",
    "Having obtained the training set i.e. a bunch of triplets (user feature vec, liked movie feature vec, not liked movie feature vec), we can train our model on it.\n",
    "\n",
    "Once triplets have been composed we can train our model. We have chosen a batch size of $16$ and $10000$ iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "evaluate_every = 100  # interval for evaluating on one-shot tasks\n",
    "batch_size = 16\n",
    "n_iter = 100000  # No. of training iterations\n",
    "n_val = 1000  # how many one-shot tasks to validate on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process!\n",
      "-------------------------------------\n",
      "\n",
      " ------------- \n",
      "\n",
      "[1151] Time for 100 iterations: 0.0 mins, Train Loss: -0.7707462906837463\n",
      "\n",
      " ------------- \n",
      "\n",
      "[1251] Time for 200 iterations: 0.1 mins, Train Loss: -0.8568305969238281\n",
      "\n",
      " ------------- \n",
      "\n",
      "[1351] Time for 300 iterations: 0.1 mins, Train Loss: -0.8862873911857605\n",
      "\n",
      " ------------- \n",
      "\n",
      "[1451] Time for 400 iterations: 0.1 mins, Train Loss: -0.6993856430053711\n",
      "\n",
      " ------------- \n",
      "\n",
      "[1551] Time for 500 iterations: 0.2 mins, Train Loss: -0.7705464959144592\n",
      "\n",
      " ------------- \n",
      "\n",
      "[1651] Time for 600 iterations: 0.2 mins, Train Loss: -0.6853159070014954\n",
      "\n",
      " ------------- \n",
      "\n",
      "[1751] Time for 700 iterations: 0.2 mins, Train Loss: -0.848142683506012\n",
      "\n",
      " ------------- \n",
      "\n",
      "[1851] Time for 800 iterations: 0.3 mins, Train Loss: -0.8554138541221619\n",
      "\n",
      " ------------- \n",
      "\n",
      "[1951] Time for 900 iterations: 0.3 mins, Train Loss: -0.7855851650238037\n",
      "\n",
      " ------------- \n",
      "\n",
      "[2051] Time for 1000 iterations: 0.3 mins, Train Loss: -0.7618976831436157\n",
      "\n",
      " ------------- \n",
      "\n",
      "[2151] Time for 1100 iterations: 0.4 mins, Train Loss: -0.7083218693733215\n",
      "\n",
      " ------------- \n",
      "\n",
      "[2251] Time for 1200 iterations: 0.4 mins, Train Loss: -0.808362603187561\n",
      "\n",
      " ------------- \n",
      "\n",
      "[2351] Time for 1300 iterations: 0.4 mins, Train Loss: -0.8481478691101074\n",
      "\n",
      " ------------- \n",
      "\n",
      "[2451] Time for 1400 iterations: 0.5 mins, Train Loss: -0.8481488227844238\n",
      "\n",
      " ------------- \n",
      "\n",
      "[2551] Time for 1500 iterations: 0.5 mins, Train Loss: -0.7704945206642151\n",
      "\n",
      " ------------- \n",
      "\n",
      "[2651] Time for 1600 iterations: 0.5 mins, Train Loss: -0.7787045240402222\n",
      "\n",
      " ------------- \n",
      "\n",
      "[2751] Time for 1700 iterations: 0.6 mins, Train Loss: -0.7492308020591736\n",
      "\n",
      " ------------- \n",
      "\n",
      "[2851] Time for 1800 iterations: 0.6 mins, Train Loss: -0.6930160522460938\n",
      "\n",
      " ------------- \n",
      "\n",
      "[2951] Time for 1900 iterations: 0.6 mins, Train Loss: -0.7705874443054199\n",
      "\n",
      " ------------- \n",
      "\n",
      "[3051] Time for 2000 iterations: 0.7 mins, Train Loss: -0.7855985164642334\n",
      "\n",
      " ------------- \n",
      "\n",
      "[3151] Time for 2100 iterations: 0.7 mins, Train Loss: -0.6994408369064331\n",
      "\n",
      " ------------- \n",
      "\n",
      "[3251] Time for 2200 iterations: 0.8 mins, Train Loss: -0.8394153714179993\n",
      "\n",
      " ------------- \n",
      "\n",
      "[3351] Time for 2300 iterations: 0.8 mins, Train Loss: -0.848156213760376\n",
      "\n",
      " ------------- \n",
      "\n",
      "[3451] Time for 2400 iterations: 0.8 mins, Train Loss: -0.7707066535949707\n",
      "\n",
      " ------------- \n",
      "\n",
      "[3551] Time for 2500 iterations: 0.9 mins, Train Loss: -0.9406641721725464\n",
      "\n",
      " ------------- \n",
      "\n",
      "[3651] Time for 2600 iterations: 0.9 mins, Train Loss: -0.7706230282783508\n",
      "\n",
      " ------------- \n",
      "\n",
      "[3751] Time for 2700 iterations: 0.9 mins, Train Loss: -0.7619836330413818\n",
      "\n",
      " ------------- \n",
      "\n",
      "[3851] Time for 2800 iterations: 1.0 mins, Train Loss: -0.7467802166938782\n",
      "\n",
      " ------------- \n",
      "\n",
      "[3951] Time for 2900 iterations: 1.0 mins, Train Loss: -0.8481578826904297\n",
      "\n",
      " ------------- \n",
      "\n",
      "[4051] Time for 3000 iterations: 1.0 mins, Train Loss: -0.8006630539894104\n",
      "\n",
      " ------------- \n",
      "\n",
      "[4151] Time for 3100 iterations: 1.1 mins, Train Loss: -0.8094164133071899\n",
      "\n",
      " ------------- \n",
      "\n",
      "[4251] Time for 3200 iterations: 1.1 mins, Train Loss: -0.8481745719909668\n",
      "\n",
      " ------------- \n",
      "\n",
      "[4351] Time for 3300 iterations: 1.1 mins, Train Loss: -0.8481558561325073\n",
      "\n",
      " ------------- \n",
      "\n",
      "[4451] Time for 3400 iterations: 1.2 mins, Train Loss: -0.8244225978851318\n",
      "\n",
      " ------------- \n",
      "\n",
      "[4551] Time for 3500 iterations: 1.2 mins, Train Loss: -0.8244234323501587\n",
      "\n",
      " ------------- \n",
      "\n",
      "[4651] Time for 3600 iterations: 1.2 mins, Train Loss: -0.8631631135940552\n",
      "\n",
      " ------------- \n",
      "\n",
      "[4751] Time for 3700 iterations: 1.3 mins, Train Loss: -0.6844178438186646\n",
      "\n",
      " ------------- \n",
      "\n",
      "[4851] Time for 3800 iterations: 1.3 mins, Train Loss: -0.8481670618057251\n",
      "\n",
      " ------------- \n",
      "\n",
      "[4951] Time for 3900 iterations: 1.3 mins, Train Loss: -0.7856717705726624\n",
      "\n",
      " ------------- \n",
      "\n",
      "[5051] Time for 4000 iterations: 1.4 mins, Train Loss: -0.8869165182113647\n",
      "\n",
      " ------------- \n",
      "\n",
      "[5151] Time for 4100 iterations: 1.4 mins, Train Loss: -0.6844239234924316\n",
      "\n",
      " ------------- \n",
      "\n",
      "[5251] Time for 4200 iterations: 1.4 mins, Train Loss: -0.7856700420379639\n",
      "\n",
      " ------------- \n",
      "\n",
      "[5351] Time for 4300 iterations: 1.5 mins, Train Loss: -0.7144412994384766\n",
      "\n",
      " ------------- \n",
      "\n",
      "[5451] Time for 4400 iterations: 1.5 mins, Train Loss: -0.8094151616096497\n",
      "\n",
      " ------------- \n",
      "\n",
      "[5551] Time for 4500 iterations: 1.6 mins, Train Loss: -0.746917724609375\n",
      "\n",
      " ------------- \n",
      "\n",
      "[5651] Time for 4600 iterations: 1.6 mins, Train Loss: -0.8006910681724548\n",
      "\n",
      " ------------- \n",
      "\n",
      "[5751] Time for 4700 iterations: 1.6 mins, Train Loss: -0.8869284391403198\n",
      "\n",
      " ------------- \n",
      "\n",
      "[5851] Time for 4800 iterations: 1.7 mins, Train Loss: -0.824429988861084\n",
      "\n",
      " ------------- \n",
      "\n",
      "[5951] Time for 4900 iterations: 1.7 mins, Train Loss: -0.7706598043441772\n",
      "\n",
      " ------------- \n",
      "\n",
      "[6051] Time for 5000 iterations: 1.7 mins, Train Loss: -0.7469111084938049\n",
      "\n",
      " ------------- \n",
      "\n",
      "[6151] Time for 5100 iterations: 1.8 mins, Train Loss: -0.6844196319580078\n",
      "\n",
      " ------------- \n",
      "\n",
      "[6251] Time for 5200 iterations: 1.8 mins, Train Loss: -0.8094128966331482\n",
      "\n",
      " ------------- \n",
      "\n",
      "[6351] Time for 5300 iterations: 1.8 mins, Train Loss: -0.8481705784797668\n",
      "\n",
      " ------------- \n",
      "\n",
      "[6451] Time for 5400 iterations: 1.9 mins, Train Loss: -0.9407026767730713\n",
      "\n",
      " ------------- \n",
      "\n",
      "[6551] Time for 5500 iterations: 1.9 mins, Train Loss: -0.746934175491333\n",
      "\n",
      " ------------- \n",
      "\n",
      "[6651] Time for 5600 iterations: 1.9 mins, Train Loss: -0.6845176815986633\n",
      "\n",
      " ------------- \n",
      "\n",
      "[6751] Time for 5700 iterations: 2.0 mins, Train Loss: -0.8285442590713501\n",
      "\n",
      " ------------- \n",
      "\n",
      "[6851] Time for 5800 iterations: 2.0 mins, Train Loss: -0.7619048357009888\n",
      "\n",
      " ------------- \n",
      "\n",
      "[6951] Time for 5900 iterations: 2.0 mins, Train Loss: -0.824432373046875\n",
      "\n",
      " ------------- \n",
      "\n",
      "[7051] Time for 6000 iterations: 2.1 mins, Train Loss: -0.8244664669036865\n",
      "\n",
      " ------------- \n",
      "\n",
      "[7151] Time for 6100 iterations: 2.1 mins, Train Loss: -0.7081654071807861\n",
      "\n",
      " ------------- \n",
      "\n",
      "[7251] Time for 6200 iterations: 2.1 mins, Train Loss: -0.6694124341011047\n",
      "\n",
      " ------------- \n",
      "\n",
      "[7351] Time for 6300 iterations: 2.2 mins, Train Loss: -0.6694046258926392\n",
      "\n",
      " ------------- \n",
      "\n",
      "[7451] Time for 6400 iterations: 2.2 mins, Train Loss: -0.7468557953834534\n",
      "\n",
      " ------------- \n",
      "\n",
      "[7551] Time for 6500 iterations: 2.2 mins, Train Loss: -0.731980562210083\n",
      "\n",
      " ------------- \n",
      "\n",
      "[7651] Time for 6600 iterations: 2.3 mins, Train Loss: -0.8628950715065002\n",
      "\n",
      " ------------- \n",
      "\n",
      "[7751] Time for 6700 iterations: 2.3 mins, Train Loss: -0.7081621885299683\n",
      "\n",
      " ------------- \n",
      "\n",
      "[7851] Time for 6800 iterations: 2.3 mins, Train Loss: -0.7775903940200806\n",
      "\n",
      " ------------- \n",
      "\n",
      "[7951] Time for 6900 iterations: 2.4 mins, Train Loss: -0.7856484055519104\n",
      "\n",
      " ------------- \n",
      "\n",
      "[8051] Time for 7000 iterations: 2.4 mins, Train Loss: -0.7707005143165588\n",
      "\n",
      " ------------- \n",
      "\n",
      "[8151] Time for 7100 iterations: 2.4 mins, Train Loss: -0.6844077110290527\n",
      "\n",
      " ------------- \n",
      "\n",
      "[8251] Time for 7200 iterations: 2.5 mins, Train Loss: -0.6219193339347839\n",
      "\n",
      " ------------- \n",
      "\n",
      "[8351] Time for 7300 iterations: 2.5 mins, Train Loss: -0.7619444727897644\n",
      "\n",
      " ------------- \n",
      "\n",
      "[8451] Time for 7400 iterations: 2.5 mins, Train Loss: -0.6219191551208496\n",
      "\n",
      " ------------- \n",
      "\n",
      "[8551] Time for 7500 iterations: 2.6 mins, Train Loss: -0.6844186782836914\n",
      "\n",
      " ------------- \n",
      "\n",
      "[8651] Time for 7600 iterations: 2.6 mins, Train Loss: -0.746918797492981\n",
      "\n",
      " ------------- \n",
      "\n",
      "[8751] Time for 7700 iterations: 2.6 mins, Train Loss: -0.7319043278694153\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training process!\")\n",
    "print(\"-------------------------------------\")\n",
    "t_start = time.time()\n",
    "\n",
    "for i in range(1, n_iter + 1):\n",
    "    triplets = get_triplets_hard(batch_size, X_usr, X_item, df_matrix)\n",
    "    loss = network_train.train_on_batch(triplets, None)\n",
    "    n_iteration += 1\n",
    "    if i % evaluate_every == 0:\n",
    "        print(\"\\n ------------- \\n\")\n",
    "        print(\n",
    "            f\"[{n_iteration}] Time for {i} iterations: {(time.time() - t_start) / 60.0:.1f} mins, Train Loss: {loss}\"\n",
    "        )\n",
    "\n",
    "# Serialise weights to HDF5\n",
    "network_train.save_weights(\"../models/network_train.h5\")\n",
    "network_predict.save_weights(\"../models/network_predict.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
